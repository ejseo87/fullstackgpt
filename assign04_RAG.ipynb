{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 717, which is longer than the specified 600\n",
      "Created a chunk of size 608, which is longer than the specified 600\n",
      "Created a chunk of size 642, which is longer than the specified 600\n",
      "Created a chunk of size 1444, which is longer than the specified 600\n",
      "Created a chunk of size 1251, which is longer than the specified 600\n",
      "Created a chunk of size 1012, which is longer than the specified 600\n",
      "Created a chunk of size 1493, which is longer than the specified 600\n",
      "Created a chunk of size 819, which is longer than the specified 600\n",
      "Created a chunk of size 1458, which is longer than the specified 600\n",
      "Created a chunk of size 1411, which is longer than the specified 600\n",
      "Created a chunk of size 742, which is longer than the specified 600\n",
      "Created a chunk of size 669, which is longer than the specified 600\n",
      "Created a chunk of size 906, which is longer than the specified 600\n",
      "Created a chunk of size 703, which is longer than the specified 600\n",
      "Created a chunk of size 1137, which is longer than the specified 600\n",
      "Created a chunk of size 1417, which is longer than the specified 600\n",
      "Created a chunk of size 1200, which is longer than the specified 600\n",
      "Created a chunk of size 859, which is longer than the specified 600\n",
      "Created a chunk of size 845, which is longer than the specified 600\n",
      "Created a chunk of size 716, which is longer than the specified 600\n",
      "Created a chunk of size 840, which is longer than the specified 600\n",
      "Created a chunk of size 1042, which is longer than the specified 600\n",
      "Created a chunk of size 652, which is longer than the specified 600\n",
      "Created a chunk of size 985, which is longer than the specified 600\n",
      "Created a chunk of size 859, which is longer than the specified 600\n",
      "Created a chunk of size 659, which is longer than the specified 600\n",
      "Created a chunk of size 693, which is longer than the specified 600\n",
      "Created a chunk of size 817, which is longer than the specified 600\n",
      "Created a chunk of size 655, which is longer than the specified 600\n",
      "Created a chunk of size 1345, which is longer than the specified 600\n",
      "Created a chunk of size 1339, which is longer than the specified 600\n",
      "Created a chunk of size 1288, which is longer than the specified 600\n",
      "Created a chunk of size 1014, which is longer than the specified 600\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 1178, which is longer than the specified 600\n",
      "Created a chunk of size 1444, which is longer than the specified 600\n",
      "Created a chunk of size 802, which is longer than the specified 600\n",
      "Created a chunk of size 1496, which is longer than the specified 600\n",
      "Created a chunk of size 841, which is longer than the specified 600\n",
      "Created a chunk of size 743, which is longer than the specified 600\n",
      "Created a chunk of size 694, which is longer than the specified 600\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  temperature=0.1,\n",
    "  streaming=True,\n",
    "  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  )\n",
    "memory = ConversationBufferMemory(\n",
    "  llm=llm,\n",
    "  return_messages=True,\n",
    ")\n",
    "#cache location\n",
    "cache_dir = LocalFileStore(\"./.cache/embeddings/assignment04\")\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "  separator = \"\\n\",\n",
    "  chunk_size = 600,\n",
    "  chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "#loading and spliiting the document\n",
    "loader = UnstructuredFileLoader(\"./.cache/files/document.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "#embedding and caching\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "  embeddings,\n",
    "  cache_dir,\n",
    ")\n",
    "vector_store = FAISS.from_documents(\n",
    "  documents=docs,\n",
    "  embedding=cached_embeddings,\n",
    ")\n",
    "#LCEL stuff chain\n",
    "retriever=vector_store.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful assistant. Answer questions using the following context. If you don't knwo the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "  MessagesPlaceholder(variable_name=\"history\"),\n",
    "  (\"user\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "  return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = {\n",
    "  \"context\": retriever, \n",
    "  \"history\": RunnableLambda(load_memory),\n",
    "  \"question\": RunnablePassthrough(),\n",
    "  } | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke(question)\n",
    "  memory.save_context({\"input\": question}, {\"output\": result.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He wrote \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE\" on the table."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What message did he write on the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia is a character mentioned in the documents provided. She is someone who the main character has strong feelings for and is willing to protect, even at his own expense."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O'Brien is angry because Winston made a statement that O'Brien found to be foolish and dangerous. This caused O'Brien to increase the level of pain inflicted on Winston using the dial."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Why O'Brien is angry?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Is Aaronson guilty?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='According to the document, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What message did he write on the table?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='He wrote \"FREEDOM IS SLAVERY\" and \"TWO AND TWO MAKE FIVE\" on the table.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Who is Julia?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Julia is a character mentioned in the documents provided. She is someone who the main character has strong feelings for and is willing to protect, even at his own expense.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"Why O'Brien is angry?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"O'Brien is angry because Winston made a statement that O'Brien found to be foolish and dangerous. This caused O'Brien to increase the level of pain inflicted on Winston using the dial.\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fullstackgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
